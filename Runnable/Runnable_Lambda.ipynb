{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b0cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the LangChain agent get fired from the library?\n",
      "\n",
      "Because it kept hallucinating new books and adding them to the catalog! The librarian said, \"We appreciate your enthusiasm, but we can't have you making up titles like 'The Existential Crisis of a Paperclip' and 'How to Knit a Black Hole'!\"\n",
      "51\n",
      "              +-------------+                \n",
      "              | PromptInput |                \n",
      "              +-------------+                \n",
      "                      *                      \n",
      "                      *                      \n",
      "                      *                      \n",
      "             +----------------+              \n",
      "             | PromptTemplate |              \n",
      "             +----------------+              \n",
      "                      *                      \n",
      "                      *                      \n",
      "                      *                      \n",
      "         +------------------------+          \n",
      "         | ChatGoogleGenerativeAI |          \n",
      "         +------------------------+          \n",
      "                      *                      \n",
      "                      *                      \n",
      "                      *                      \n",
      "            +-----------------+              \n",
      "            | StrOutputParser |              \n",
      "            +-----------------+              \n",
      "                      *                      \n",
      "                      *                      \n",
      "                      *                      \n",
      "  +-------------------------------------+    \n",
      "  | Parallel<Joke,Joke Word Count>Input |    \n",
      "  +-------------------------------------+    \n",
      "              **            ***              \n",
      "            **                 **            \n",
      "          **                     **          \n",
      "+-------------+              +------------+  \n",
      "| Passthrough |              | word_count |  \n",
      "+-------------+              +------------+  \n",
      "              **            ***              \n",
      "                **        **                 \n",
      "                  **    **                   \n",
      "  +--------------------------------------+   \n",
      "  | Parallel<Joke,Joke Word Count>Output |   \n",
      "  +--------------------------------------+   \n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableSequence,RunnablePassthrough ,RunnableLambda\n",
    "load_dotenv()\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"Generate a joke about {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "parser=StrOutputParser()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "\n",
    "chain=RunnableSequence(\n",
    "    RunnableSequence(prompt1, model, parser),  # joke is generated here\n",
    "    RunnableParallel({\n",
    "        'Joke': RunnablePassthrough(),  # joke is passed through here\n",
    "        'Joke Word Count': RunnableLambda(word_count),  # word count is calculated here\n",
    "    })\n",
    ")\n",
    "result=chain.invoke({\"topic\":\"LangChain\"})\n",
    "\n",
    "print(result['Joke'])\n",
    "\n",
    "print(result['Joke Word Count'])\n",
    "\n",
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
