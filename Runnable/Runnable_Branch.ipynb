{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6d1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Chain for Topic: LangChain ---\n",
      "\n",
      "LangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It addresses the inherent limitations of raw LLMs (like limited context, lack of external knowledge, and inability to perform actions) by providing a modular and structured approach to connect them with external data sources, computational tools, and memory.\n",
      "\n",
      "The framework is built around six core components:\n",
      "1.  **Models:** Interfaces to various LLM providers.\n",
      "2.  **Prompts:** Tools for constructing and managing LLM inputs.\n",
      "3.  **Chains:** Sequences of operations, enabling multi-step reasoning and workflows (e.g., using LangChain Expression Language - LCEL).\n",
      "4.  **Retrieval:** Methods for integrating external data (like documents from vector stores) into the LLM's context, crucial for Retrieval Augmented Generation (RAG).\n",
      "5.  **Memory:** Mechanisms to persist state and conversation history across interactions.\n",
      "6.  **Agents:** LLMs that dynamically decide which tools to use and in what order to achieve a goal.\n",
      "\n",
      "LangChain facilitates the creation of advanced applications such as sophisticated chatbots, question-answering systems over custom documents, content generators, and autonomous agents. Its benefits include modularity, abstraction, simplified orchestration, extensibility, and rapid prototyping. However, it also presents challenges like a steep learning curve, potential performance overhead, and debugging complexity (though tools like LangSmith help).\n",
      "\n",
      "The framework is supported by a growing ecosystem, including LangSmith for debugging and evaluation, and LangServe for deployment, and is continuously evolving to enhance agent capabilities and production readiness. Ultimately, LangChain serves as a foundational and indispensable tool for building complex, real-world LLM-powered applications.\n",
      "\n",
      "--- End of Output ---\n",
      "\n",
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "        +--------+         \n",
      "        | Branch |         \n",
      "        +--------+         \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "     +--------------+      \n",
      "     | BranchOutput |      \n",
      "     +--------------+      \n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "# Load environment variables (e.g., API key)\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define Prompts ---\n",
    "\n",
    "# Prompt to generate the detailed report\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"Write a detailed report on {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Prompt to summarize the report\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Summarize the following report in a concise manner: {text}.\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# --- 2. Define Model and Parser ---\n",
    "\n",
    "parser = StrOutputParser()\n",
    "# Assuming 'gemini-2.0-flash' is accessible; adjusting to a current available model name\n",
    "# If this still causes an error, check your API key and model availability.\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
    "\n",
    "# --- 3. Define Conditional Logic and Chains ---\n",
    "\n",
    "# This inner chain is executed when the report IS longer than 500 words.\n",
    "# FIX: The RunnableLambda maps the incoming string (the report text)\n",
    "# to the dictionary key 'text' expected by prompt2.\n",
    "summarization_chain = RunnableSequence(\n",
    "    # Map the incoming string 'text' to the dictionary {'text': text}\n",
    "    RunnableLambda(lambda text: {\"text\": text}),\n",
    "    prompt2,\n",
    "    model,\n",
    "    parser\n",
    ")\n",
    "\n",
    "# Define the branching logic\n",
    "# FIX: Changed the second conditional pair to be the default RunnablePassThrough()\n",
    "# If the first condition (> 500 words) is True, the summarization_chain runs.\n",
    "# Otherwise (<= 500 words), the default RunnablePassthrough() runs.\n",
    "branch = RunnableBranch(\n",
    "    # True Condition: Report is too long (> 500 words) -> Summarize it\n",
    "    (lambda x: len(x.split()) > 500, summarization_chain),\n",
    "\n",
    "    # Default/Else Condition: Report is short enough (<= 500 words) -> Pass it through\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "# The main chain: (Generate Report) -> (Conditional Branch)\n",
    "chain = RunnableSequence(\n",
    "    RunnableSequence(prompt1, model, parser).with_config(run_name=\"Generate Report\"),\n",
    "    branch.with_config(run_name=\"Word Count Check & Action\")\n",
    ")\n",
    "\n",
    "# --- 4. Invoke and Print ---\n",
    "\n",
    "# Running the chain (the actual output depends on the model's response length)\n",
    "print(f\"--- Running Chain for Topic: LangChain ---\\n\")\n",
    "print(chain.invoke({\"topic\": \"LangChain\"}))\n",
    "print(f\"\\n--- End of Output ---\\n\")\n",
    "\n",
    "# Print the ASCII graph for visualization\n",
    "chain.get_graph().print_ascii()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
